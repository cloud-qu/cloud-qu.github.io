<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yun Qu | 曲云</title>

    <meta name="author" content="Yun Qu | 曲云">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yun Qu | 曲云
                </p>
                <p style="line-height:1.7;">
                  I'm a fourth-year Ph.D. student in the Department of Automation at <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>, advised by <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Prof. Xiangyang Ji</a>.
                  My research focuses on <strong>Reinforcement Learning</strong> and <strong>Large Language Models</strong>.
I work with the <a href="https://www.thuidm.com/">THU-IDM</a> team, where we develop efficient algorithms for decision-making.
Prior to my doctoral studies, I received my B.E. degree from the Department of Automation at Tsinghua University.

                </p>
                <p style="text-align:center">
                  <a href="mailto:qy22@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=l9Ky9goAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/quyun52425662">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/cloud-qu">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;box-shadow:0 2px 8px rgba(0,0,0,0.15);" alt="profile photo" src="qyimages/YunQu.jpg">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="border-bottom:2px solid #e0e0e0;padding-bottom:10px;">News</h2>
                <div style="max-height:200px;overflow-y:auto;margin-top:15px;">
                <ul style="margin:0;padding-left:20px;">
                  <li style='margin-bottom:10px;line-height:1.6;'><strong style='color:#2c5aa0;'>[2025-09]</strong> One paper accepted to <strong>NeurIPS 2025</strong>!</li>
                  <li style='margin-bottom:10px;line-height:1.6;'><strong style='color:#2c5aa0;'>[2025-05]</strong> <a href='https://arxiv.org/abs/2504.19139'>PDTS</a> was accepted to <strong>ICML 2025</strong>!</li>
                  <li style='margin-bottom:10px;line-height:1.6;'><strong style='color:#2c5aa0;'>[2024-12]</strong> <a href='https://arxiv.org/abs/2412.11120'>LaRe</a> was accepted to <strong>AAAI 2025</strong>.</li>
                  <li style='margin-bottom:10px;line-height:1.6;'><strong style='color:#2c5aa0;'>[2024-09]</strong> Two papers accepted to <strong>NeurIPS 2024</strong>!</li>
                  <li style='margin-bottom:10px;line-height:1.6;'><strong style='color:#2c5aa0;'>[2022-09]</strong> Started my Ph.D. journey at Tsinghua University.</li>
                </ul>
                </div>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="border-bottom:2px solid #e0e0e0;padding-bottom:10px;">Research</h2>
                <p style="line-height:1.7;color:#555;margin-top:15px;">
                  I'm interested in <strong>Reinforcement Learning</strong> and <strong>Large Language Models</strong>. My research focuses on efficient and intelligent decision-making with minimal environment interactions. Representative works are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/mopps.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/mopps.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2507.04632">
          <span class="papertitle">Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <strong>Yun Qu</strong>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://taohu.me/">Vincent Tao Hu</a>,
        <a href="https://scholar.google.de/citations?user=zWbvIUcAAAAJ">Björn Ommer</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>arxiv</em>, 2025
        <br>
        <span style="font-size:14px;">
        <a href="https://arxiv.org/abs/2507.04632">paper</a>
        /
        <a href="https://github.com/thu-rllab/MoPPS">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        This work introduces Model Predictive Prompt Selection, a Bayesian risk-predictive framework that online estimates prompt difficulty without requiring costly LLM interactions.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/PDTS.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/PDTS.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2504.19139">
          <span class="papertitle">Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <strong>Yun Qu*</strong>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao*</a>,
        Yiqin Lv,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>ICML</em>, 2025
        <br>
        <span style="font-size:14px;">
        <a href="https://arxiv.org/abs/2504.19139">paper</a>
        /
        <a href="https://github.com/thu-rllab/PDTS">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        We propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/MPTS.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/MPTS.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://autumn9999.github.io/">
          <span class="papertitle">Model Predictive Task Sampling for Efficient and Robust Adaptation</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        <a href="https://zzzx1224.github.io/">Zehao Xiao*</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao*</a>,
        <strong>Yun Qu*</strong>,
        <a href="https://autumn9999.github.io/">Jiayi Shen</a>,
        Yiqin Lv,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>arxiv</em>, 2025
        <br>
        <span style="font-size:14px;">
        <a href="https://autumn9999.github.io/">paper</a>
        /
        <a href="https://github.com/thu-rllab/MPTS">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        We introduce Model Predictive Task Sampling (MPTS), a framework that bridges the task space and adaptation risk landscape, providing a theoretical foundation for robust active task sampling.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/LaRe.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/LaRe.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2412.11120">
          <span class="papertitle">Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement Learning</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <strong>Yun Qu*</strong>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang*</a>,
        <a href="https://scholar.google.com/citations?user=BQyuCvwAAAAJ">Boyuan Wang</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        Chang Liu,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>AAAI</em>, 2025
        <br>
        <span style="font-size:14px;">
        <a href="https://arxiv.org/abs/2412.11120">paper</a>
        /
        <a href="https://github.com/thu-rllab/LaRe">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        We introduce LaRe, a novel LLM-empowered symbolic-based decision-making framework, to improve credit assignment in episodic reinforcement learning.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2411.07934">
          <span class="papertitle">Doubly Mild Generalization for Offline Reinforcement Learning</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang</a>,
        <strong>Yun Qu</strong>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <span style="font-size:14px;">
        <a href="https://arxiv.org/abs/2411.07934">paper</a>
        /
        <a href="https://github.com/maoyixiu/DMG">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        To appropriately exploit generalization in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild action generalization and (ii) mild generalization propagation.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/SCAS.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/SCAS.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2410.19400">
          <span class="papertitle">Offline reinforcement learning with ood state correction and ood action suppression</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang</a>,
        Chen Chen,
        <strong>Yun Qu</strong>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <span style="font-size:14px;">
        <a href="https://arxiv.org/abs/2410.19400">paper</a>
        /
        <a href="https://github.com/maoyixiu/SCAS">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        We propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/LEMAE.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/LEMAE.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2410.02511">
          <span class="papertitle">Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <strong>Yun Qu</strong>,
        <a href="https://scholar.google.com/citations?user=BQyuCvwAAAAJ">Boyuan Wang</a>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        <a href="https://scholar.google.com/citations?user=uCLTRH8AAAAJ">Jianzhun Shao</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        Chang Liu,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>arxiv</em>, 2024
        <br>
        <span style="font-size:14px;">
        <a href="https://arxiv.org/abs/2410.02511">paper</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        This paper introduces a systematic approach, termed LEMAE, choosing to channel informative task-relevant guidance from a knowledgeable Large Language Model (LLM) for Efficient Multi-Agent Exploration.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/ARMAML.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/ARMAML.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/ar-metalearn">
          <span class="papertitle">Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        Yiqin Lv*,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao*</a>,
        <strong>Yun Qu</strong>,
        <a href="https://yxu71.github.io/">Yi Xu</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>KDD</em>, 2025
        <br>
        <span style="font-size:14px;">
        <a href="https://sites.google.com/view/ar-metalearn">project page</a>
        /
        <a href="https://dl.acm.org/doi/abs/10.1145/3690624.3709337">paper</a>
        /
        <a href="https://github.com/lvyiqin/AR-MAML">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        We consider explicitly generative modeling task distributions placed over task identifiers and propose robustifying fast adaptation from adversarial training.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/LESR.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/LESR.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2407.13237">
          <span class="papertitle">LLM-Empowered State Representation for Reinforcement Learning</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <a href="https://scholar.google.com/citations?user=BQyuCvwAAAAJ">Boyuan Wang*</a>,
        <strong>Yun Qu*</strong>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        Chang Liu,
        Wenming Yang,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>ICML</em>, 2024
        <br>
        <span style="font-size:14px;">
        <a href="https://arxiv.org/abs/2407.13237">paper</a>
        /
        <a href="https://github.com/thu-rllab/LESR">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        We propose LLM-Empowered State Representation (LESR), a novel approach that utilizes LLM to autonomously generate task-related state representation codes which help to enhance the continuity of network mappings and facilitate efficient training.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/hokoff.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/hokoff.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/hok-offline">
          <span class="papertitle">Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <strong>Yun Qu*</strong>,
        <a href="https://scholar.google.com/citations?user=BQyuCvwAAAAJ">Boyuan Wang*</a>,
        <a href="https://scholar.google.com/citations?user=uCLTRH8AAAAJ">Jianzhun Shao*</a>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        Chen Chen,
        Zhenbin Ye,
        Linc Liu,
        Yang Feng,
        Lin Lai,
        Hongyang Qin,
        Minwen Deng,
        Juchao Zhuo,
        Deheng Ye,
        Qiang Fu,
        Yang Guang,
        Wei Yang,
        Lanxiao Huang,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>NeurIPS D&B Track</em>, 2023
        <br>
        <span style="font-size:14px;">
        <a href="https://sites.google.com/view/hok-offline">project page</a>
        /
        <a href="https://arxiv.org/abs/2408.10556">paper</a>
        /
        <a href="https://github.com/tencent-ailab/hokoff">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        We propose Hokoff, a comprehensive set of pre-collected datasets that covers both offline RL and offline MARL, accompanied by a robust framework, to facilitate further research.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/CFCQL.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/CFCQL.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2309.12696">
          <span class="papertitle">Counterfactual Conservative Q Learning for Offline Multi-Agent Reinforcement Learning</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <a href="https://scholar.google.com/citations?user=uCLTRH8AAAAJ">Jianzhun Shao*</a>,
        <strong>Yun Qu*</strong>,
        Chen Chen,
        Hongchang Zhang,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>NeurIPS</em>, 2023
        <br>
        <span style="font-size:14px;">
        <a href="https://arxiv.org/abs/2309.12696">paper</a>
        /
        <a href="https://github.com/thu-rllab/CFCQL">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        We propose a novel multi-agent offline RL algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct conservative value estimation.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div style="border-radius:4px;overflow:hidden;box-shadow:0 1px 3px rgba(0,0,0,0.12);">
          <img src='qyimages/CAMA.png' width="160" style="width:100%;display:block;cursor:pointer;transition:transform 0.2s;" 
               onmouseover="this.style.transform='scale(1.05)'" 
               onmouseout="this.style.transform='scale(1)'"
               onclick="openLightbox('qyimages/CAMA.png')">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://proceedings.mlr.press/v202/shao23b/shao23b.pdf">
          <span class="papertitle">Complementary Attention for Multi-Agent Reinforcement Learning</span>
        </a>
        <br>
        <span style="font-size:14px;color:#555;">
        <a href="https://scholar.google.com/citations?user=uCLTRH8AAAAJ">Jianzhun Shao</a>,
        Hongchang Zhang,
        <strong>Yun Qu</strong>,
        Chang Liu,
        Shuncheng He,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        </span>
        <br>
        <em>ICML</em>, 2023
        <br>
        <span style="font-size:14px;">
        <a href="https://proceedings.mlr.press/v202/shao23b/shao23b.pdf">paper</a>
        /
        <a href="https://github.com/thu-rllab/CAMA">code</a>
        </span>
        <p></p>
        <p style="color:#666;font-size:14px;line-height:1.6;">
        In this paper, we propose Complementary Attention for Multi-Agent reinforcement learning (CAMA), which applies a divide-and-conquer strategy on input entities accompanied with the complementary attention of enhancement and replenishment.
        </p>
      </td>
    </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="border-bottom:2px solid #e0e0e0;padding-bottom:10px;">Miscellaneous</h2>
                <h3 style='color:#333;margin-top:20px;margin-bottom:10px;font-weight:600;'>Academic Service</h3>
                <ul style='line-height:1.8;margin:10px 0;padding-left:20px;'>
                  <li style='color:#555;margin-bottom:8px;'>Reviewer for NeurIPS, ICML, ICLR, AAAI</li>
                  <li style='color:#555;margin-bottom:8px;'>Reviewer for Expert Systems With Applications</li>
                </ul>
                <h3 style='color:#333;margin-top:20px;margin-bottom:10px;font-weight:600;'>Awards & Honors</h3>
                <ul style='line-height:1.8;margin:10px 0;padding-left:20px;'>
                  <li style='color:#555;margin-bottom:8px;'>Outstanding Graduate Award, Beijing, 2022</li>
                  <li style='color:#555;margin-bottom:8px;'>Outstanding Graduate Award, Tsinghua University, 2022</li>
                </ul>
                <h3 style='color:#333;margin-top:20px;margin-bottom:10px;font-weight:600;'>Collaborations</h3>
                <p style='color:#666;line-height:1.7;margin:10px 0;'>I'm fortunate to collaborate with researchers from <a href='https://www.thuidm.com/'>THU-IDM</a> and other institutions.</p>
              </td>
            </tr>
          </tbody></table>

    <!-- Lightbox Modal -->
    <div id="lightbox" style="display:none;position:fixed;z-index:9999;left:0;top:0;width:100%;height:100%;background-color:rgba(0,0,0,0.9);cursor:pointer;" onclick="closeLightbox()">
      <span style="position:absolute;top:20px;right:40px;color:#f1f1f1;font-size:40px;font-weight:bold;cursor:pointer;" onclick="closeLightbox()">&times;</span>
      <img id="lightbox-img" style="margin:auto;display:block;max-width:90%;max-height:90%;position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);box-shadow:0 4px 20px rgba(0,0,0,0.5);">
    </div>
    
    <script>
    function openLightbox(imgSrc) {
      event.stopPropagation();
      document.getElementById('lightbox').style.display = 'block';
      document.getElementById('lightbox-img').src = imgSrc;
      document.body.style.overflow = 'hidden';
    }
    
    function closeLightbox() {
      document.getElementById('lightbox').style.display = 'none';
      document.body.style.overflow = 'auto';
    }
    
    // 按ESC键关闭
    document.addEventListener('keydown', function(e) {
      if (e.key === 'Escape') {
        closeLightbox();
      }
    });
    </script>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;color:#999;">
                  Website template from <a href="https://jonbarron.info/" style="color:#999;">Jon Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>

  </body>
</html>