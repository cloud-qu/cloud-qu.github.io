<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yun Qu</title>

    <meta name="author" content="Yun Qu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yun Qu
                </p>
                <p>
                  I'm a Ph.D. candidate in the Department of Automation at <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>, advised by <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Prof. Xiangyang Ji</a>.
                  My research focuses on <strong>Reinforcement Learning</strong> and <strong>Large Language Models</strong>.
I work with the <a href="https://www.thuidm.com/">THU-IDM</a> team, where we develop efficient algorithms for decision-making.
Prior to my doctoral studies, I received my B.Eng. degree from the Department of Automation at Tsinghua University.

                </p>
                <p style="text-align:center">
                  <a href="mailto:qy22@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=l9Ky9goAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/quyun52425662">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/cloud-qu">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="qyimages/YunQu.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="qyimages/YunQu.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in <strong>Reinforcement Learning</strong> and <strong>Large Language Models</strong>. My research focuses on efficient and intelligent decision-making with minimal environment interactions. Representative works are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/mopps.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?</span>
        </a>
        <br>
        <strong>Yun Qu</strong>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://taohu.me/">Vincent Tao Hu</a>,
        <a href="https://scholar.google.de/citations?user=zWbvIUcAAAAJ">Bj√∂rn Ommer</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>arxiv</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2507.04632">paper</a>
        /
        <a href="https://github.com/thu-rllab/MoPPS">code</a>
        <p></p>
        <p>
        This work introduces Model Predictive Prompt Selection, a Bayesian risk-predictive framework that online estimates prompt difficulty without requiring costly LLM interactions.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/PDTS.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments</span>
        </a>
        <br>
        <strong>Yun Qu*</strong>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao*</a>,
        <a href="">Yiqin Lv</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>ICML</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2504.19139">paper</a>
        /
        <a href="https://github.com/thu-rllab/PDTS">code</a>
        <p></p>
        <p>
        We propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/MPTS.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Model Predictive Task Sampling for Efficient and Robust Adaptation</span>
        </a>
        <br>
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        <a href="https://zzzx1224.github.io/">Zehao Xiao*</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao*</a>,
        <strong>Yun Qu*</strong>,
        <a href="https://autumn9999.github.io/">Jiayi Shen</a>,
        <a href="">Yiqin Lv</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>arxiv</em>, 2025
        <br>
        <a href="https://autumn9999.github.io/">paper</a>
        /
        <a href="https://github.com/thu-rllab/MPTS">code</a>
        <p></p>
        <p>
        We introduce Model Predictive Task Sampling (MPTS), a framework that bridges the task space and adaptation risk landscape, providing a theoretical foundation for robust active task sampling.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/LaRe.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement Learning</span>
        </a>
        <br>
        <strong>Yun Qu*</strong>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang*</a>,
        <a href="https://scholar.google.com/citations?user=BQyuCvwAAAAJ">Boyuan Wang</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        <a href="">Chang Liu</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>AAAI</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2412.11120">paper</a>
        /
        <a href="https://github.com/thu-rllab/LaRe">code</a>
        <p></p>
        <p>
        We introduce LaRe, a novel LLM-empowered symbolic-based decision-making framework, to improve credit assignment in episodic reinforcement learning.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Doubly Mild Generalization for Offline Reinforcement Learning</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang</a>,
        <strong>Yun Qu</strong>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2411.07934">paper</a>
        /
        <a href="https://github.com/maoyixiu/DMG">code</a>
        <p></p>
        <p>
        To appropriately exploit generalization in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild action generalization and (ii) mild generalization propagation.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/SCAS.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Offline reinforcement learning with ood state correction and ood action suppression</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang</a>,
        <a href="">Chen Chen</a>,
        <strong>Yun Qu</strong>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2410.19400">paper</a>
        /
        <a href="https://github.com/maoyixiu/SCAS">code</a>
        <p></p>
        <p>
        We propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/LEMAE.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration</span>
        </a>
        <br>
        <strong>Yun Qu</strong>,
        <a href="https://scholar.google.com/citations?user=BQyuCvwAAAAJ">Boyuan Wang</a>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        <a href="https://scholar.google.com/citations?user=uCLTRH8AAAAJ">Jianzhun Shao</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao</a>,
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        <a href="">Chang Liu</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>arxiv</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2410.02511">paper</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        This paper introduces a systematic approach, termed LEMAE, choosing to channel informative task-relevant guidance from a knowledgeable Large Language Model (LLM) for Efficient Multi-Agent Exploration.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/ARMAML.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://sites.google.com/view/ar-metalearn">
          <span class="papertitle">Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation</span>
        </a>
        <br>
        <a href="https://sites.google.com/view/albert-q-wang-at-ai-community/home">Qi (Cheems) Wang*</a>,
        <a href="">Yiqin Lv*</a>,
        <a href="https://scholar.google.com/citations?user=gNcb5LUAAAAJ">Yixiu Mao*</a>,
        <strong>Yun Qu</strong>,
        <a href="https://yxu71.github.io/">Yi Xu</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>KDD</em>, 2025
        <br>
        <a href="https://sites.google.com/view/ar-metalearn">project page</a>
        /
        <a href="https://dl.acm.org/doi/abs/10.1145/3690624.3709337">paper</a>
        /
        <a href="https://github.com/lvyiqin/AR-MAML">code</a>
        <p></p>
        <p>
        We consider explicitly generative modeling task distributions placed over task identifiers and propose robustifying fast adaptation from adversarial training.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/LESR.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">LLM-Empowered State Representation for Reinforcement Learning</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=BQyuCvwAAAAJ">Boyuan Wang*</a>,
        <strong>Yun Qu*</strong>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        <a href="">Chang Liu</a>,
        <a href="">Wenming Yang</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>ICML</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2407.13237">paper</a>
        /
        <a href="https://github.com/thu-rllab/LESR">code</a>
        <p></p>
        <p>
        We propose LLM-Empowered State Representation (LESR), a novel approach that utilizes LLM to autonomously generate task-related state representation codes which help to enhance the continuity of network mappings and facilitate efficient training.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/hokoff.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://sites.google.com/view/hok-offline">
          <span class="papertitle">Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks</span>
        </a>
        <br>
        <strong>Yun Qu*</strong>,
        <a href="https://scholar.google.com/citations?user=BQyuCvwAAAAJ">Boyuan Wang*</a>,
        <a href="https://scholar.google.com/citations?user=uCLTRH8AAAAJ">Jianzhun Shao*</a>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        <a href="">Chen Chen</a>,
        <a href="">Zhenbin Ye</a>,
        <a href="">Linc Liu</a>,
        <a href="">Yang Feng</a>,
        <a href="">Lin Lai</a>,
        <a href="">Hongyang Qin</a>,
        <a href="">Minwen Deng</a>,
        <a href="">Juchao Zhuo</a>,
        <a href="">Deheng Ye</a>,
        <a href="">Qiang Fu</a>,
        <a href="">Yang Guang</a>,
        <a href="">Wei Yang</a>,
        <a href="">Lanxiao Huang</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>NeurIPS D&B Track</em>, 2023
        <br>
        <a href="https://sites.google.com/view/hok-offline">project page</a>
        /
        <a href="https://arxiv.org/abs/2408.10556">paper</a>
        /
        <a href="https://github.com/tencent-ailab/hokoff">code</a>
        <p></p>
        <p>
        We propose Hokoff, a comprehensive set of pre-collected datasets that covers both offline RL and offline MARL, accompanied by a robust framework, to facilitate further research.
        </p>
      </td>
    </tr>

    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/CFCQL.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Counterfactual Conservative Q Learning for Offline Multi-Agent Reinforcement Learning</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=uCLTRH8AAAAJ">Jianzhun Shao*</a>,
        <strong>Yun Qu*</strong>,
        <a href="">Chen Chen</a>,
        <a href="">Hongchang Zhang</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>NeurIPS</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2309.12696">paper</a>
        /
        <a href="https://github.com/thu-rllab/CFCQL">code</a>
        <p></p>
        <p>
        We propose a novel multi-agent offline RL algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct conservative value estimation.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='qyimages/CAMA.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Complementary Attention for Multi-Agent Reinforcement Learning</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=uCLTRH8AAAAJ">Jianzhun Shao</a>,
        <a href="">Hongchang Zhang</a>,
        <strong>Yun Qu</strong>,
        <a href="">Chang Liu</a>,
        <a href="">Shuncheng He</a>,
        <a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ">Yuhang Jiang</a>,
        <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm">Xiangyang Ji</a>
        <br>
        <em>ICML</em>, 2023
        <br>
        <a href="https://proceedings.mlr.press/v202/shao23b/shao23b.pdf">paper</a>
        /
        <a href="https://github.com/thu-rllab/CAMA">code</a>
        <p></p>
        <p>
        In this paper, we propose Complementary Attention for Multi-Agent reinforcement learning (CAMA), which applies a divide-and-conquer strategy on input entities accompanied with the complementary attention of enhancement and replenishment.
        </p>
      </td>
    </tr>

          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template from <a href="https://jonbarron.info/">Jon Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
